{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task-1**"
      ],
      "metadata": {
        "id": "1WldvhAzUQlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bQS7cKYSUKd7"
      },
      "outputs": [],
      "source": [
        "def min_edit_distance(s1, s2):\n",
        "\n",
        "    m = len(s1)\n",
        "    n = len(s2)\n",
        "\n",
        "\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "\n",
        "\n",
        "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
        "\n",
        "\n",
        "            dp[i][j] = min(dp[i - 1][j] + 1,\n",
        "                           dp[i][j - 1] + 1,\n",
        "                           dp[i - 1][j - 1] + cost)\n",
        "\n",
        "\n",
        "    return dp[m][n]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(str1, str2):\n",
        "    distance = min_edit_distance(str1, str2)\n",
        "    max_len = max(len(str1), len(str2))\n",
        "    if max_len == 0:\n",
        "        return 1.0\n",
        "    return 1 - (distance / max_len)\n",
        "\n",
        "def main():\n",
        "    print(\"String Similarity Calculator (with DP Table)\")\n",
        "    str1 = input(\"Enter first string: \")\n",
        "    str2 = input(\"Enter second string: \")\n",
        "\n",
        "    score = similarity_score(str1, str2)\n",
        "    print(f\"\\nSimilarity Score: {score:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_jQM_G1Vjv_",
        "outputId": "3c1c3b30-0b19-4ba9-a718-184f8e093d3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String Similarity Calculator (with DP Table)\n",
            "Enter first string: dwd\n",
            "Enter second string: greg\n",
            "\n",
            "Similarity Score: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task-2**"
      ],
      "metadata": {
        "id": "_wi4AwZOUdEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Load Corpus"
      ],
      "metadata": {
        "id": "71K7xF9yU5J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import os\n",
        "\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "\n",
        "sentences = brown.sents()\n",
        "sentences = [[w.lower() for w in sent] for sent in sentences]\n",
        "\n",
        "print(\"Total sentences in Brown Corpus:\", len(sentences))\n",
        "print(\"Example sentence:\", sentences[0])"
      ],
      "metadata": {
        "id": "Bzd7S76vUgy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build unigram, bigram, trigram counts"
      ],
      "metadata": {
        "id": "v2AworQhU9KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "unigram_counts = Counter()\n",
        "bigram_counts = Counter()\n",
        "trigram_counts = Counter()\n",
        "\n",
        "for sent in sentences:\n",
        "    sent = [\"<s>\"] + sent + [\"</s>\"]\n",
        "    for i in range(len(sent)):\n",
        "        unigram_counts[sent[i]] += 1\n",
        "    for i in range(len(sent)-1):\n",
        "        bigram_counts[(sent[i], sent[i+1])] += 1\n",
        "    for i in range(len(sent)-2):\n",
        "        trigram_counts[(sent[i], sent[i+1], sent[i+2])] += 1\n",
        "\n",
        "vocab = len(unigram_counts)\n",
        "\n",
        "most_freq_word = unigram_counts.most_common(1)[0][0]\n",
        "\n",
        "print(\"Vocabulary size:\", vocab)\n",
        "print(\"Most frequent word:\", most_freq_word)"
      ],
      "metadata": {
        "id": "XcjSUnhLUkrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict next word safely"
      ],
      "metadata": {
        "id": "pkMBEdVDU_fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram(word):\n",
        "    candidates = {w2: count for (w1, w2), count in bigram_counts.items() if w1 == word}\n",
        "    if candidates:\n",
        "        return max(candidates, key=candidates.get)\n",
        "    else:\n",
        "        return most_freq_word\n",
        "\n",
        "def predict_next_word_trigram(word1, word2):\n",
        "    candidates = {w3: count for (w1, w2_, w3), count in trigram_counts.items() if w1 == word1 and w2_ == word2}\n",
        "    if candidates:\n",
        "        return max(candidates, key=candidates.get)\n",
        "    else:\n",
        "        return predict_next_word_bigram(word2)"
      ],
      "metadata": {
        "id": "B3hEPqhtUqHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show n-grams"
      ],
      "metadata": {
        "id": "ImmMMyRzVDj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def show_ngrams(sentence):\n",
        "\n",
        "    nltk.data.path.append(os.path.join(os.path.expanduser(\"~\"), \"nltk_data\"))\n",
        "    tokens = nltk.word_tokenize(sentence.lower())\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "    trigrams = list(nltk.trigrams(tokens))\n",
        "    return bigrams, trigrams\n",
        "\n",
        "\n",
        "sentence = \"i am eating rice\"\n",
        "bigrams, trigrams = show_ngrams(sentence)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)"
      ],
      "metadata": {
        "id": "hsix1aTxUt_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence probability using plain Laplace smoothing"
      ],
      "metadata": {
        "id": "_4ayw3WmVJ8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_probability_laplace(sentence):\n",
        "    tokens = [\"<s>\"] + nltk.word_tokenize(sentence.lower()) + [\"</s>\"]\n",
        "    prob = 1.0\n",
        "    for i in range(len(tokens)-1):\n",
        "        w1, w2 = tokens[i], tokens[i+1]\n",
        "        count_bigram = bigram_counts.get((w1, w2), 0)\n",
        "        count_unigram = unigram_counts.get(w1, 0)\n",
        "        prob *= (count_bigram + 1) / (count_unigram + vocab)\n",
        "    return prob\n",
        "\n",
        "def trigram_probability_laplace(sentence):\n",
        "    tokens = [\"<s>\"] + nltk.word_tokenize(sentence.lower()) + [\"</s>\"]\n",
        "    prob = 1.0\n",
        "    for i in range(len(tokens)-2):\n",
        "        w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "        count_trigram = trigram_counts.get((w1, w2, w3), 0)\n",
        "        count_bigram = bigram_counts.get((w1, w2), 0)\n",
        "        prob *= (count_trigram + 1) / (count_bigram + vocab)\n",
        "    return prob"
      ],
      "metadata": {
        "id": "bQH2uew2U1cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Input Loop and probability using laplace smothing"
      ],
      "metadata": {
        "id": "o6d7eYHnVPWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    print(\"\\nEnter a sentence (or type 'exit' to quit):\")\n",
        "    user_input = input()\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    tokens = nltk.word_tokenize(user_input.lower())\n",
        "    print(\"Tokens:\", tokens)\n",
        "\n",
        "\n",
        "    if len(tokens) >= 2:\n",
        "        next_word = predict_next_word_trigram(tokens[-2], tokens[-1])\n",
        "    else:\n",
        "        next_word = predict_next_word_bigram(tokens[-1])\n",
        "    print(\"Predicted next word:\", next_word)\n",
        "\n",
        "\n",
        "    bigrams, trigrams = show_ngrams(user_input)\n",
        "    print(\"Bigrams:\", bigrams)\n",
        "    print(\"Trigrams:\", trigrams)\n",
        "\n",
        "\n",
        "    bigram_prob = bigram_probability_laplace(user_input)\n",
        "    trigram_prob = trigram_probability_laplace(user_input)\n",
        "    print(f\"Bigram Probability (Laplace): {bigram_prob:.10e}\")\n",
        "    print(f\"Trigram Probability (Laplace): {trigram_prob:.10e}\")"
      ],
      "metadata": {
        "id": "FtRjgGp2U3FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"--- Installing necessary libraries ---\")\n",
        "!pip install transformers torch pandas scikit-learn tqdm -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Connecting to Google Drive ---\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NOTEEVENTS_random.csv'\n",
        "\n",
        "\n",
        "print(f\"\\n--- Attempting to load data from: {file_path} ---\")\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: File not found at '{file_path}'\")\n",
        "else:\n",
        "    df_notes = pd.read_csv(file_path, low_memory=False)\n",
        "    print(\"Data loaded successfully!\")\n",
        "\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        text = re.sub(r'\\n', ' ', text); text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip(); return text.lower()\n",
        "\n",
        "    df_notes['TEXT'] = df_notes['TEXT'].apply(clean_text)\n",
        "    print(\"Text cleaning complete.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Preparing datasets for two tasks ---\")\n",
        "\n",
        "    df_physician = df_notes[df_notes['CATEGORY'] == 'Physician '].copy()\n",
        "    df_nursing = df_notes[df_notes['CATEGORY'] == 'Nursing/other'].copy()\n",
        "    df_physician['label'] = 0; df_nursing['label'] = 1\n",
        "    sample_size_task1 = min(len(df_physician), len(df_nursing), 1500)\n",
        "    df_task1 = pd.concat([df_physician.sample(sample_size_task1, random_state=42), df_nursing.sample(sample_size_task1, random_state=42)]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Task 1 (Physician vs Nursing) Dataset Size: {len(df_task1)}\")\n",
        "    print(df_task1['label'].value_counts())\n",
        "\n",
        "\n",
        "    def is_mortality_note(text):\n",
        "        mortality_keywords = ['comfort measures', 'comfort care', 'palliative care', 'hospice', 'withdrew care', 'withdrawal of support', 'terminal extubation']\n",
        "        return 1 if any(keyword in text for keyword in mortality_keywords) else 0\n",
        "\n",
        "    df_notes['label'] = df_notes['TEXT'].apply(is_mortality_note)\n",
        "    df_mortality_positive = df_notes[df_notes['label'] == 1].copy()\n",
        "    df_mortality_negative = df_notes[df_notes['label'] == 0].copy()\n",
        "\n",
        "    df_mortality_negative_sampled = df_mortality_negative.sample(len(df_mortality_positive), random_state=42)\n",
        "    df_task2_full = pd.concat([df_mortality_positive, df_mortality_negative_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    df_task2 = df_task2_full.sample(n=min(len(df_task2_full), 3000), random_state=42).reset_index(drop=True)\n",
        "    print(f\"\\nTask 2 (Mortality Prediction) Dataset Size: {len(df_task2)} (Sampled from {len(df_task2_full)} total)\")\n",
        "    print(df_task2['label'].value_counts())\n",
        "\n",
        "\n",
        "    class MedicalNotesDataset(Dataset):\n",
        "        def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "            self.texts, self.labels, self.tokenizer, self.max_len = texts, labels, tokenizer, max_len\n",
        "        def __len__(self): return len(self.texts)\n",
        "        def __getitem__(self, item):\n",
        "            text, label = str(self.texts[item]), self.labels[item]\n",
        "            encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt')\n",
        "            return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "    def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "        ds = MedicalNotesDataset(texts=df.TEXT.to_numpy(), labels=df.label.to_numpy(), tokenizer=tokenizer, max_len=max_len)\n",
        "        return DataLoader(ds, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
        "        model = model.train(); losses, correct_predictions = [], 0\n",
        "        for d in data_loader:\n",
        "            input_ids, attention_mask, labels = d[\"input_ids\"].to(device), d[\"attention_mask\"].to(device), d[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss; _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels); losses.append(loss.item())\n",
        "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
        "        return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "    def eval_model(model, data_loader, device):\n",
        "        model = model.eval(); losses, correct_predictions = [], 0; all_labels, all_preds = [], []\n",
        "        with torch.no_grad():\n",
        "            for d in data_loader:\n",
        "                input_ids, attention_mask, labels = d[\"input_ids\"].to(device), d[\"attention_mask\"].to(device), d[\"labels\"].to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                _, preds = torch.max(outputs.logits, dim=1)\n",
        "                correct_predictions += torch.sum(preds == labels); losses.append(outputs.loss.item())\n",
        "                all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n",
        "        accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
        "        return accuracy, np.mean(losses), precision, recall, f1\n",
        "\n",
        "    def run_experiment(df, model_name, task_name):\n",
        "        print(f\"\\n{'='*25} RUNNING EXPERIMENT {'='*25}\")\n",
        "        print(f\"TASK: {task_name} | MODEL: {model_name}\")\n",
        "        MAX_LEN, BATCH_SIZE, EPOCHS, LEARNING_RATE = 256, 16, 4, 2e-5\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "        train_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "        test_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=len(train_loader) * EPOCHS)\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_acc, train_loss = train_epoch(model, train_loader, optimizer, device, scheduler)\n",
        "            print(f'Epoch {epoch + 1}/{EPOCHS} -> Train loss {train_loss:.4f}, accuracy {train_acc:.4f}')\n",
        "        test_acc, _, precision, recall, f1 = eval_model(model, test_loader, device)\n",
        "        print(f\"-> FINAL TEST RESULTS: Accuracy: {test_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "        return {'task': task_name, 'model': model_name.split('/')[-1], 'f1_score': f1, 'accuracy': test_acc.item()}\n",
        "\n",
        "\n",
        "    models_to_compare = ['bert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n",
        "    all_results = []\n",
        "\n",
        "\n",
        "    for model_name in models_to_compare:\n",
        "        result = run_experiment(df_task1, model_name, \"Physician vs Nursing\")\n",
        "        all_results.append(result)\n",
        "\n",
        "    for model_name in models_to_compare:\n",
        "        result = run_experiment(df_task2, model_name, \"Mortality Prediction\")\n",
        "        all_results.append(result)\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" FINAL COMPARATIVE RESULTS \" + \"=\"*20)\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    print(results_df.pivot_table(index='task', columns='model', values=['f1_score', 'accuracy']))\n",
        "    print(\"\\n\" + \"=\"*65)\n",
        "\n",
        "\n",
        "data = {\n",
        "    'Task': ['Physician vs Nursing', 'Mortality Prediction'],\n",
        "    'BERT-base Accuracy': [1.00, 0.91],\n",
        "    'Bio_ClinicalBERT Accuracy': [1.00, 0.9217],\n",
        "    'BERT-base F1': [1.00, 0.9135],\n",
        "    'Bio_ClinicalBERT F1': [1.00, 0.9246]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df_melted = df.melt(id_vars='Task', var_name='Metric_Model', value_name='Score')\n",
        "df_melted[['Model', 'Metric']] = df_melted['Metric_Model'].str.extract(r'([A-Za-z_]+)\\s*(Accuracy|F1)')\n",
        "df_melted.drop('Metric_Model', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.barplot(\n",
        "    data=df_melted,\n",
        "    x='Task',\n",
        "    y='Score',\n",
        "    hue='Model',\n",
        "    palette=['#66b3ff', '#99ff99'],\n",
        "    errorbar=None\n",
        ")\n",
        "\n",
        "plt.title('Accuracy & F1 Comparison for BERT Models', fontsize=14, pad=15)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Task')\n",
        "plt.ylim(0.85, 1.05)\n",
        "plt.legend(title='Model', loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df_heatmap = pd.DataFrame({\n",
        "    'Task': ['Physician vs Nursing', 'Physician vs Nursing', 'Mortality Prediction', 'Mortality Prediction'],\n",
        "    'Metric': ['Accuracy', 'F1', 'Accuracy', 'F1'],\n",
        "    'BERT-base': [1.00, 1.00, 0.91, 0.9135],\n",
        "    'Bio_ClinicalBERT': [1.00, 1.00, 0.9217, 0.9246]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.heatmap(\n",
        "    df_heatmap.pivot(index='Task', columns='Metric', values='BERT-base'),\n",
        "    annot=True, fmt='.3f', cmap='Blues', cbar=False, linewidths=1\n",
        ")\n",
        "plt.title('BERT-base Performance Heatmap', fontsize=13, pad=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.heatmap(\n",
        "    df_heatmap.pivot(index='Task', columns='Metric', values='Bio_ClinicalBERT'),\n",
        "    annot=True, fmt='.3f', cmap='Greens', cbar=False, linewidths=1\n",
        ")\n",
        "plt.title('Bio_ClinicalBERT Performance Heatmap', fontsize=13, pad=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.lineplot(\n",
        "    data=df_melted,\n",
        "    x='Task',\n",
        "    y='Score',\n",
        "    hue='Model',\n",
        "    style='Metric',\n",
        "    markers=True,\n",
        "    dashes=False,\n",
        "    linewidth=2.5,\n",
        "    palette=['#1f77b4', '#2ca02c']\n",
        ")\n",
        "\n",
        "plt.title('Performance Trend: Accuracy vs F1 across Tasks', fontsize=14, pad=10)\n",
        "plt.ylim(0.85, 1.05)\n",
        "plt.ylabel('Score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gDdLwoSdWVEG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}